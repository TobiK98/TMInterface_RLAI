# import TM stuff
import sys
from time import time
from tminterface.interface import TMInterface
from tminterface.client import Client, run_client

# import others
import numpy as np


class Q_Agent(Client):
    def __init__(self) -> None:
        super(Q_Agent, self).__init__()
        self.state = np.array([])
        self.starting_state = np.array([])
        self.prev_state = np.array([])
        self.all_states = np.array([])
        self.current_state_index = 0
        self.actions = np.arange(4)
        self.q = np.zeros([1, 4])
        self.learning_rate = 0.1
        self.epsilon = 0.9
        self.epsilon_decay = 0.98
        self.discount = 0.95
        self.current_episode = 0
        self.episodes = 100
        self.episode_reward = 0
        self.rewards = np.array([])
        self.cp_states = np.zeros(8)
        self.prev_cp_states = np.zeros(8)

    # register to TM
    def on_registered(self, iface: TMInterface) -> None:
        print(f'Registered to {iface.server_name}')
        self.starting_state = iface.get_simulation_state().position
        self.state = self.starting_state

    # step
    def on_run_step(self, iface: TMInterface, _time: int):
        if _time >= 0 and _time % 50 == 0:

            if self.state not in self.all_states:
                self.all_states = np.append(self.all_states, self.state)
                self.q = np.vstack((self.q, np.zeros(4)))

            self.current_state_index = np.where(self.all_states == self.state)

            if np.random.random() < self.epsilon:
                action = np.random.choice(self.actions)
            else:
                action = np.argmax(self.q[self.current_state_index])

            if action == 0:
                iface.set_input_state(left=True, accelerate=True, right=False, brake=False)
            if action == 1:
                iface.set_input_state(left=False, accelerate=True, right=False, brake=False)
            if action == 2:
                iface.set_input_state(left=False, accelerate=True, right=True, brake=False)
            if action == 3:
                iface.set_input_state(left=False, accelerate=True, right=False, brake=True)

            self.prev_state = self.state
            game_data = iface.get_simulation_state()
            self.state = game_data.position

            self.prev_cp_states = self.cp_states
            self.cp_states = iface.get_checkpoint_state().cp_states
            reward = 0
            if sum(self.cp_states) > sum(self.prev_cp_states):
                reward += +25
            elif sum(self.cp_states) == 8:
                reward += +100
            else:
                reward = -0.5
            self.episode_reward += reward

            current_q = self.q[self.current_state_index][action]
            max_future_q = np.max(self.q[self.current_state_index])
            new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.discount * max_future_q)
            self.q[self.current_state_index][action] = new_q

            if _time == 20000 or sum(self.cp_states) == 8:
                print(f'===== Finished episode {self.current_episode} =====')
                print(f'Episode reward: {self.episode_reward}')
                self.rewards = np.append(self.rewards, self.episode_reward)
                print(f'Average reward: {np.mean(self.rewards)}')
                self.state = self.starting_state
                self.current_episode += 1
                self.episode_reward = 0
                self.epsilon *= self.epsilon_decay
                iface.give_up()

            

    # let game play when goal reached
    def on_checkpoint_count_changed(self, iface: TMInterface, current: int, target: int):
        if current == target:
            iface.prevent_simulation_finish()



agent = Q_Agent()

# connect to TM
def main():
    server_name = f'TMInterface{sys.argv[1]}' if len(sys.argv) > 1 else 'TMInterface0'
    print(f'Connecting to {server_name}...')
    run_client(agent, server_name)

if __name__ == '__main__':
    main()